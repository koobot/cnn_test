{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch GD\n",
    "p. 115 of hands on machine learning with scikit-learn and tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 2 * np.random.rand(100, 1) # 100 x 1 array\n",
    "y = 4 + 3 * X + np.random.rand(100, 1) # linear equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think 4 = constant and np.random.rand is the residual error. Should probably double check this somewhere..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create array with x0 for constant\n",
    "X_b = np.c_[np.ones(shape=(100, 1)), X] # add x0 = 1 to each instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ],\n",
       "       [0.38900758, 0.12315012, 0.48918445, 1.65132536, 0.70290896,\n",
       "        1.30877731, 1.8125573 , 1.33417805, 0.51502424, 1.12728132,\n",
       "        1.63651255, 1.05979687, 0.62151358, 0.27367121, 1.69604109,\n",
       "        1.35636068, 0.86510614, 1.73424401, 0.43030253, 1.00896066,\n",
       "        1.62001514, 0.2863425 , 1.30160728, 0.3379844 , 1.91004049,\n",
       "        0.92714673, 1.81117398, 0.55083423, 1.67126722, 0.8875823 ,\n",
       "        1.27414986, 1.93546303, 1.25225252, 1.08356453, 0.2131713 ,\n",
       "        0.61224573, 1.56222313, 1.95136887, 1.44116038, 1.26390686,\n",
       "        0.68468233, 0.79227112, 0.74369245, 1.7680539 , 0.14981122,\n",
       "        1.89675998, 1.47609247, 1.76964848, 0.9752984 , 1.8600535 ,\n",
       "        1.14219805, 1.40971923, 0.70989692, 1.23102396, 0.24166149,\n",
       "        0.84146875, 0.14521769, 1.61201039, 0.34158677, 0.54249899,\n",
       "        0.09472957, 1.84800887, 1.66494691, 1.86752284, 0.80089374,\n",
       "        1.72372832, 1.75433168, 1.87514598, 1.51846383, 0.0347898 ,\n",
       "        1.63525592, 1.29029442, 1.08719214, 0.87796204, 0.30674942,\n",
       "        1.23511608, 1.21913498, 1.60014792, 1.87019313, 0.88836129,\n",
       "        1.61723188, 0.7708405 , 1.54415714, 1.95497267, 1.37365306,\n",
       "        0.68893505, 0.49910047, 0.3919295 , 0.89674004, 0.28667133,\n",
       "        1.2215539 , 1.80076135, 0.36991734, 0.01193251, 0.68707007,\n",
       "        0.02785382, 0.68157115, 1.84643451, 0.28060955, 0.32056699]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can see transposed:\n",
    "X_b.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1 # learning rate\n",
    "n_iterations = 1000\n",
    "m = 100 # How many rows/observations\n",
    "\n",
    "theta = np.random.randn(2, 1) # random initialisation 2x1 array\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.47690812],\n",
       "       [3.01923756]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.47690812],\n",
       "       [3.01923756]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normal equation - p.109\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can see the gradient batch descent method (iterative), gets same param results as normal equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens if I change m\n",
    "m is number of instance/observations/rows of dataset (could be training or test, depending on what I'm modelling on).\n",
    "\n",
    "Actually I shouldn't really be changing the m, since this was set earlier when I created the data set with array 100 x 1.\n",
    "\n",
    "Oh well, fun to experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.60354508],\n",
       "       [1.20854741]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta = 0.1 # learning rate\n",
    "n_iterations = 1000\n",
    "m = 100000\n",
    "\n",
    "theta = np.random.randn(2, 1) # random initialisation 2x1 array\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "    \n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.62936189e+127],\n",
       "       [-2.04212899e+127]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta = 0.5 # learning rate\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "\n",
    "theta = np.random.randn(2, 1) # random initialisation 2x1 array\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "    \n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.63908813],\n",
       "       [3.67551297]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta = 0.1 # learning rate\n",
    "n_iterations = 10\n",
    "m = 100\n",
    "\n",
    "theta = np.random.randn(2, 1) # random initialisation 2x1 array\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "    \n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD\n",
    "p.118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.48495165],\n",
       "       [3.03213169]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 100 # sample size\n",
    "n_epochs = 50\n",
    "t0, t1 = 5, 50 # learning schedule hyperparameters\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "theta = np.random.randn(2, 1) # random initialisation\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index:random_index+1] # select one random instance\n",
    "        yi = y[random_index:random_index+1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        theta = theta - eta * gradients\n",
    "        \n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random thought - if `X_b[81]` is the same as `X_b[81:82]` then why can't I just shorten the code to `X_b[random_index]`?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-naive_bayes]",
   "language": "python",
   "name": "conda-env-.conda-naive_bayes-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
